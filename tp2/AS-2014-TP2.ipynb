{
 "metadata": {
  "name": "AS-2014-TP2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TP2 : Programmation du Perceptron\n",
      "=======\n",
      "\n",
      "Dans ce TP, nous allons principalement programmer un perceptron, et mettre en place une \"architecture\" de code nous permettant petit \u00e0 petit d'impl\u00e9menter des Deep Neural Networks. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Etape 1: Dataset\n",
      "------\n",
      "\n",
      "La premi\u00e8re \u00e9tape consiste \u00e0 d\u00e9finir une classe permettant de stocker les donn\u00e9es d'apprentissage, de validation et de test. Nous consid\u00e9rerons que les donn\u00e9es tiennent en m\u00e9moire. Nous allons d\u00e9finir une classe permettant de stocker des couples $\\{(x_1,y_1),...,(x_n,y_n)\\}$. Les $x_i$ et $y_i$ seront des tableaux numpy "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class LabeledSet:  \n",
      "    \n",
      "    def __init__(self,x,y,input_dim,output_dim):\n",
      "        self.x = x\n",
      "        self.y = y\n",
      "        self.input_dim = input_dim\n",
      "        self.output_dim = output_dim\n",
      "    \n",
      "    #Renvoie la dimension de l'espace d'entr\u00e9e\n",
      "    def getInputDimension(self):\n",
      "        return self.input_dim\n",
      "       \n",
      "    \n",
      "    #Renvoie la dimension de l'espace de sortie\n",
      "    def getOutputDimension(self):\n",
      "        return self.output_dim\n",
      "    \n",
      "    #Renvoie le nombre d'exemple dans le set\n",
      "    def size(self):\n",
      "        return len(self.x)\n",
      "    #Renvoie la valeur de x_i\n",
      "    def getX(self,i):\n",
      "        return self.x[i]\n",
      "        \n",
      "    \n",
      "    #Renvouie la valeur de y_i\n",
      "    def getY(self,i):\n",
      "        return self.y[1]\n",
      "       \n",
      "  \n",
      "\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 68
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nous allons pour l'instant nous int\u00e9resser \u00e0 des datasets \"jouet\" g\u00e9n\u00e9res selon des distributions choisies \u00e0 la main. Commen\u00e7ons par un dataset en 2 dimensions (entr\u00e9e) et 1 dimension (sortie): $x_i \\in \\mathbb{R}^2$, $y_i \\in [-1;+1]$ telle que les donn\u00e9es sont g\u00e9n\u00e9res selon deux Gaussiennes. Pour cela, nous utiliserons la fonction numpy.random.multivariate_normal  - http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.multivariate_normal.html -  ainsi que la m\u00e9thode numpy.vstack  - http://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html - pour concaterner des vecteurs "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def createGaussianDataset(positive_center_1,positive_center_2,positive_sigma,negative_center_1,negative_center_2,negative_sigma,nb_points):\n",
      "    pos = True\n",
      "    first = True\n",
      "    \n",
      "    while nb_points>0:\n",
      "        \n",
      "        if pos:\n",
      "            a = np.random.multivariate_normal([positive_center_1,positive_center_2],[[positive_sigma,0],[0,positive_sigma]])\n",
      "            if first:\n",
      "                x=a\n",
      "                first = False\n",
      "                y = np.array([1])\n",
      "            else:\n",
      "                x = np.vstack((x,a))\n",
      "                y = np.vstack((y,np.array([1])))\n",
      "                pos = False\n",
      "        else:\n",
      "            b = np.random.multivariate_normal([negative_center_1,negative_center_2],[[negative_sigma,0],[0,negative_sigma]])\n",
      "            x = np.vstack((x,b))\n",
      "            y = np.vstack((y,np.array([-1])))\n",
      "            pos = True\n",
      "        \n",
      "        \n",
      "        nb_points -= 1\n",
      "       \n",
      "         \n",
      "    return LabeledSet(x,y,2,1)\n",
      "\n",
      "setGauss = createGaussianDataset(1,2,4,-5,-2,3,200)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Le data set peut \u00eatre affich\u00e9 en utilisatn matplotlib (pour v\u00e9rifier). Nous utiliserons la commande matplotlib.pyplot.scatter permettant de dessiner un nuage de points - http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter -"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "def plot2DSet(set):\n",
      "    plt.scatter(set.x[:,0],set.x[:,1])\n",
      "    plt.show()\n",
      "\n",
      "plot2DSet(setGauss)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(200, 2)\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Maintenant, nous allons faire la m\u00eame chose, mais en dessinant une fronti\u00e8re de d\u00e9cision donn\u00e9e par une fonction $f : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^1$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def  plot_frontiere(x,f,step=20):\n",
      "    mmax=x.max(0)\n",
      "    mmin=x.min(0)\n",
      "    x1grid,x2grid=np.meshgrid(np.linspace(mmin[0],mmax[0],step),np.linspace(mmin[1],mmax[1],step))\n",
      "    grid=np.hstack((x1grid.reshape(x1grid.size,1),x2grid.reshape(x2grid.size,1)))\n",
      "    \n",
      "    # calcul de la prediction pour chaque point de la grille\n",
      "    res=np.array([f(grid[i,:])[0] for i in range(len(grid)) ])\n",
      "    res=res.reshape(x1grid.shape)\n",
      "    # tracer des frontieres\n",
      "    plt.contourf(x1grid,x2grid,res,colors=[\"orange\",\"gray\"],levels=[-1000,0,1000],linewidth=2)\n",
      "\n",
      "\n",
      "def f(x):\n",
      "    score=[x[0]+x[1]]\n",
      "    return(score)\n",
      "\n",
      "set=createGaussianDataset(1,1,1,-1,-1,1,200)\n",
      "\n",
      "x=set.x\n",
      "y=set.y\n",
      "plot_frontiere(x,f)\n",
      "plot2DSet(set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Etape 2 : Le Perceptron\n",
      "--------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nous allons commencer par cr\u00e9er une classe permettant de d\u00e9finir un pr\u00e9dicteur. Basiquement, un pr\u00e9dicteur est une fonction qui prend un vecteur et produit un vecteur, et que l'on peut entrainer sur un \"dataset\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class Predictor:\n",
      "    def __init__(self):\n",
      "        raise NotImplementedError(\"Please Implement this method\")\n",
      "    \n",
      "    \n",
      "    #Permet de calculer la prediction sur x\n",
      "    def predict(self,x):\n",
      "        raise NotImplementedError(\"Please Implement this method\")\n",
      "\n",
      "    \n",
      "    #Permet d'entrainer le modele\n",
      "    def train(self,labeledSet):\n",
      "        raise NotImplementedError(\"Please Implement this method\")\n",
      "    \n",
      "    #Permet de calculer la qualit\u00e9 du syst\u00e8me (en classification monolabel). ATTENTION, deux cas: outputDimension==1 et outputDimension>1\n",
      "    def computeMonolabelAccuracy(self,labeledset):\n",
      "        prediction = self.predict(labeledset.x)\n",
      "        nbError = 0\n",
      "        if labeledset.getOutputDimension() == 1:\n",
      "            \n",
      "            for predicted,original in zip(prediction,labeledset.y):\n",
      "                print predicted\n",
      "                print original\n",
      "                if predicted != original:\n",
      "                    \n",
      "                    nbError += 1\n",
      "          \n",
      "        else:\n",
      "             raise NotImplementedError(\"Please Implement this part\")\n",
      "        \n",
      "        \n",
      "        return nbError/labeledset.size()*100\n",
      "                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Le premier classifieur \u00e0 impl\u00e9menter sera le classifieur perceptron vu en cours"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "class Perceptron(Predictor):\n",
      "    def __init__(self,gradient_step,nb_iterations):\n",
      "        self.gradient_step = gradient_step\n",
      "        self.nb_iterations = nb_iterations\n",
      "        \n",
      "    def predict(self,x):\n",
      "        return x*self.w.T\n",
      "        \n",
      "    def train(self,labeledSet):\n",
      "        self.w = np.ones(labeledSet.getInputDimension())\n",
      "        \n",
      "        for i in xrange(0,self.nb_iterations):\n",
      "            ind = np.random.random_integers(0, high=labeledSet.size())\n",
      "            ex = labeledSet.getX(ind)\n",
      "            lb = labeledSet.getY(ind)\n",
      "            self.w = self.gradient_step*(lb*(ex*self.w))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 136
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On va maintenant tester notre perceptron sur l'ensemble pr\u00e9c\u00e9dent"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainset=createGaussianDataset(1,1,1,-1,-1,1,200)\n",
      "perceptron=Perceptron(0.001,10)\n",
      "\n",
      "perceptron.train(trainset)\n",
      "perceptron.w\n",
      "perceptron.computeMonolabelAccuracy(trainset)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-137-79c124fbd97b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mperceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mperceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeMonolabelAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-135-58557a5abe23>\u001b[0m in \u001b[0;36mcomputeMonolabelAccuracy\u001b[0;34m(self, labeledset)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mnbError\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ -3.44806796e-32  -2.37722874e-31]\n",
        "[1]\n"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualiser la frontiere de d\u00e9cision obtenus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Etape 3 : Tester le Perceptron\n",
      "------\n",
      "On va maintenant cr\u00e9er des ensembles de test et de train selon plusieurs distribution et tracer les courbes d'apprentissage (accuraacy) sur train et test dans le temps. Pouvez vous faire apparaitre un effet de sur-apprentissage? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pouvez vous faire apparaitre un cas ou le perceptron ne peut pas apprendre"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Etape 4 : Kernel Trick\n",
      "-----\n",
      "On va transformer nos vecteurs d'entr\u00e9e de la mani\u00e8re suivante: $(x_1,x_2) \\rightarrow $(x_1,x_2,1)$. Apprenez le perceptron sur le nouvel ensemble et visualisez sa frontiere de d\u00e9cision. Que pouvez vous dire ? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "On va transformer nos vecteurs d'entr\u00e9e de la mani\u00e8re suivante: $(x_1,x_2) \\rightarrow $(x_1,x_2,1,x_1*x_1,x_2*x_2,x_1*x_2)$. Apprenez le perceptron sur le nouvel ensemble et visualisez sa frontiere de d\u00e9cision. Que pouvez vous dire ? Pourquoi observe-t-on cette fronti\u00e8re de d\u00e9cision ? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Etape 5 : UCI\n",
      "-----\n",
      "\n",
      "Plusieurs datasets sont t\u00e9l\u00e9chargeables ici:  http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\n",
      "\n",
      "* Impl\u00e9mentez une fonction permettant de les charger\n",
      "* Impl\u00e9mnetez une fonction permettant de les \"splitter\" en train et test\n",
      "* Lancer les diff\u00e9rentes perceptrons la dessus et tracer les courbes de performance\n",
      "\n",
      "(Datasets conseill\u00e9s: sonar, jeart, breast-cancer et ionosphere)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}